# lecture-ai-engineering-day3

宿題について
   
概要
この宿題では、講義で学んだRAG(Retrieval-Augmented Generation)技術を用いて、LLMの生成
内容を改善する実践的な取り組みを行います。演習で利用したコードをベースに、独自の質問と参照文書を用
いて実験を行い、RAGの効果を定量的・定性的に評価します。
この宿題を通じて、「テストデータの作成」と「改善のプロセス」について理解を深め、実際のアプリケーション開発
に役立てることを目指します。

宿題の内容
1. 独自の質問と参照資料の作成
○ 自分で5つ以上の質問文を考案してください
○ 各質問に対する回答を含む参照文書を用意してください
○ 少なくとも1つは、LLMが単体では正確に答えられないような知識を含む質問にしてください
2. 実験の実施
○ 演習で使用したコードをベースに、以下の2つの方法で回答を生成してください
■ ベースのLLM(RAGなし)での回答生成
■ RAGを組み合わせた回答生成
○ 回答の評価では、単純なYes/No判定でも良いです
■ より詳細な評価指標も検討していただけるとなお良いです

3. 結果分析と考察
○ 生成した結果をまとめ、RAGありとRAGなしの差異を分析してください
○ RAGによって回答が改善したケースと悪化したケースの両方について考察してください
○ 結果に基づいて、RAGの有効性と限界についての考察を記述してください

提出方法
1. 以下の内容を含む HOMEWORK.md ファイルを作成してください:
○ 質問設計の観点と意図
○ RAGの実装方法と工夫点
○ 結果の分析と考察
○ 発展的な改善案(任意)
2. 作成したコード、質問、結果をリポジトリにまとめ、提出してください。
評価指標の設計(発展的な宿題)
回答の評価方法をより詳細に検討する。例えば:
● 正確性: 回答が事実と一致しているか(0-5点)
● 完全性: 質問に対する必要な情報がすべて含まれているか(0-5点)
● 関連性: 回答が質問に適切に対応しているか(0-5点)
● 自動評価: LLMによるYes/No評価や数値評価の設計

ヒントとアドバイス
● 質問設計: 単純な事実確認だけでなく、複数の情報を組み合わせる必要がある質問や、最新情報が必要な質問など、多様な質
問タイプを検討してください
● 参照文書: 質問に直接関連する情報と、あえて関連性の低い情報を混ぜることで、RAGの検索精度も評価できます
● 評価手法: 人間による主観評価とLLMによる自動評価の両方を活用する
● 実験の工夫: チャンク分割サイズやエンベディングモデルなど、異なるパラメータでの実験する
提出期限
● 提出期限: 2025年5月7日 17時まで
● 提出方法: 第1回、第2回と同様の方法で提出していただく予定です。詳細は別途案内します。

